{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Shaheer-Ali89/ML-Project/blob/main/ML_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Ehh5CbfL_8iG"
      },
      "outputs": [],
      "source": [
        "# Classical ML Imports\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.utils.data as data\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nNyDqcZWAxV0"
      },
      "outputs": [],
      "source": [
        "# Transform: Resize to 32x32 + ToTensor\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((32, 32)),\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "train_data = datasets.FashionMNIST(root='./data', train=True, download=True, transform=transform)\n",
        "test_data = datasets.FashionMNIST(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "train_loader = data.DataLoader(train_data, batch_size=256, shuffle=True)\n",
        "test_loader = data.DataLoader(test_data, batch_size=256, shuffle=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0RPn1t1aA0Y7"
      },
      "outputs": [],
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "class MLPEncoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Linear(32*32, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 32)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.flatten(x)\n",
        "        return self.encoder(x)\n",
        "\n",
        "encoder = MLPEncoder().to(device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gDayUMQsNkb4"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5XV9-DzJA7EF"
      },
      "outputs": [],
      "source": [
        "# Train Encoder (simple feature learning)\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "encoder.to(device)\n",
        "\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = torch.optim.Adam(encoder.parameters(), lr=1e-3)\n",
        "\n",
        "for epoch in range(5):\n",
        "    for imgs, _ in train_loader:\n",
        "        imgs = imgs.to(device)\n",
        "\n",
        "        # --- Prepare target as flattened image ---\n",
        "        target = imgs.view(imgs.size(0), -1)   # shape: (batch, 1024)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        features = encoder(imgs)               # shape: (batch, 32)\n",
        "\n",
        "        # --- Expand features to match target size ---\n",
        "        # simple linear projection\n",
        "        proj = nn.Linear(32, 1024).to(device)\n",
        "        proj_output = proj(features)\n",
        "\n",
        "        loss = criterion(proj_output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    print(f\"Epoch {epoch+1} Loss: {loss.item():.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SF_5UtW5BB48"
      },
      "outputs": [],
      "source": [
        "#Extract Features\n",
        "def extract_features(dataloader):\n",
        "    features = []\n",
        "    labels = []\n",
        "    with torch.no_grad():\n",
        "        for imgs, lbls in dataloader:\n",
        "            imgs = imgs.to(device)\n",
        "            f = encoder(imgs).cpu().numpy()\n",
        "            features.append(f)\n",
        "            labels.append(lbls.numpy())\n",
        "    return np.vstack(features), np.hstack(labels)\n",
        "\n",
        "X_train, y_train = extract_features(train_loader)\n",
        "X_test, y_test = extract_features(test_loader)\n",
        "print(\"Feature shape:\", X_train.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "450gi5e6BKff"
      },
      "outputs": [],
      "source": [
        "#Train Classical Models\n",
        "# 1. Support Vector Machine\n",
        "svm = SVC(kernel='rbf')\n",
        "svm.fit(X_train, y_train)\n",
        "svm_preds = svm.predict(X_test)\n",
        "\n",
        "# 2. Random Forest\n",
        "rf = RandomForestClassifier(n_estimators=150)\n",
        "rf.fit(X_train, y_train)\n",
        "rf_preds = rf.predict(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qt_sG2VoBati"
      },
      "outputs": [],
      "source": [
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# -------------------------------\n",
        "# Load dataset with correct 32x32 transform\n",
        "# -------------------------------\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((32, 32)),\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "test_set = datasets.FashionMNIST(\n",
        "    root=\"data\",\n",
        "    train=False,\n",
        "    download=True,\n",
        "    transform=transform\n",
        ")\n",
        "\n",
        "labels_map = {\n",
        "    0: \"T-shirt/top\",\n",
        "    1: \"Trouser\",\n",
        "    2: \"Pullover\",\n",
        "    3: \"Dress\",\n",
        "    4: \"Coat\",\n",
        "    5: \"Sandal\",\n",
        "    6: \"Shirt\",\n",
        "    7: \"Sneaker\",\n",
        "    8: \"Bag\",\n",
        "    9: \"Ankle boot\",\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yxn4K0tzBPMa"
      },
      "outputs": [],
      "source": [
        "# =============================\n",
        "# Visualization + Prediction\n",
        "# =============================\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Pick sample\n",
        "sample_img, sample_label = test_set[0]\n",
        "\n",
        "# Show the image\n",
        "plt.imshow(sample_img.squeeze(), cmap=\"gray\")\n",
        "plt.title(f\"True Label: {labels_map[sample_label]}\")\n",
        "plt.axis(\"off\")\n",
        "plt.show()\n",
        "\n",
        "# Prepare correct shape for encoder\n",
        "sample = sample_img.unsqueeze(0).to(device)   # (1, 1, 32, 32)\n",
        "\n",
        "# Pass through encoder\n",
        "with torch.no_grad():\n",
        "    encoded = encoder(sample)                 # (1, 32)\n",
        "\n",
        "# Convert feature to numpy\n",
        "feat = encoded.cpu().numpy()\n",
        "\n",
        "# Predictions\n",
        "svm_pred = svm.predict(feat)[0]\n",
        "rf_pred  = rf.predict(feat)[0]\n",
        "\n",
        "print(\"SVM Prediction:\", labels_map[svm_pred])\n",
        "print(\"RF Prediction:\",  labels_map[rf_pred])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JstCbAu_BhQ4"
      },
      "outputs": [],
      "source": [
        "# ==========================\n",
        "# Correct CNN Dataset Loader\n",
        "# (MUST match 32Ã—32 encoder)\n",
        "# ==========================\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((32, 32)),   # <<< IMPORTANT\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "train_set = datasets.FashionMNIST(root='./data', train=True, download=True, transform=transform)\n",
        "test_set = datasets.FashionMNIST(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "train_loader = DataLoader(train_set, batch_size=64, shuffle=True)\n",
        "test_loader = DataLoader(test_set, batch_size=64, shuffle=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_EpTRrkKBlJP"
      },
      "outputs": [],
      "source": [
        "# Define CNN Model for 32x32 input\n",
        "class CNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.conv_layers = nn.Sequential(\n",
        "            nn.Conv2d(1, 32, 3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2),   # 32 -> 16\n",
        "\n",
        "            nn.Conv2d(32, 64, 3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2)    # 16 -> 8\n",
        "        )\n",
        "\n",
        "        self.fc_layers = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(64 * 8 * 8, 128),   # updated for 32x32\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(128, 10)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv_layers(x)\n",
        "        return self.fc_layers(x)\n",
        "\n",
        "model = CNN().to(device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N-bL6KljBvsq",
        "outputId": "a7ed65ef-c04d-448b-85ea-66b72d5a527c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10 - Loss: 0.5128\n",
            "Epoch 2/10 - Loss: 0.3371\n",
            "Epoch 3/10 - Loss: 0.2888\n",
            "Epoch 4/10 - Loss: 0.2619\n",
            "Epoch 5/10 - Loss: 0.2388\n",
            "Epoch 6/10 - Loss: 0.2204\n",
            "Epoch 7/10 - Loss: 0.2029\n",
            "Epoch 8/10 - Loss: 0.1876\n",
            "Epoch 9/10 - Loss: 0.1752\n",
            "Epoch 10/10 - Loss: 0.1604\n"
          ]
        }
      ],
      "source": [
        "#Train Model\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "epochs = 10\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    for imgs, labels in train_loader:\n",
        "        imgs, labels = imgs.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(imgs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{epochs} - Loss: {total_loss/len(train_loader):.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yAL-iSzHBzMa",
        "outputId": "a02048ee-dfc3-4d7b-8b79-4219ee64b727"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Accuracy: 0.9166\n"
          ]
        }
      ],
      "source": [
        "#Evaluate\n",
        "model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for imgs, labels in test_loader:\n",
        "        imgs, labels = imgs.to(device), labels.to(device)\n",
        "        outputs = model(imgs)\n",
        "        preds = torch.argmax(outputs, dim=1)\n",
        "\n",
        "        correct += (preds == labels).sum().item()\n",
        "        total += labels.size(0)\n",
        "\n",
        "print(\"CNN Accuracy:\", correct / total)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 445
        },
        "id": "BjbfGwvFCG5M",
        "outputId": "7b6d7995-261e-424c-9438-9e8b797ac3ce"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Prediction: Ankle boot\n"
          ]
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Pick one image from the test set\n",
        "sample_img, sample_label = test_set[0]\n",
        "\n",
        "plt.imshow(sample_img.squeeze(), cmap=\"gray\")\n",
        "plt.title(f\"True Label: {labels_map[sample_label]}\")\n",
        "plt.axis(\"off\")\n",
        "plt.show()\n",
        "\n",
        "# Prepare for CNN\n",
        "sample = sample_img.unsqueeze(0).to(device)\n",
        "\n",
        "# Predict\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    output = model(sample)\n",
        "    pred = torch.argmax(F.softmax(output, dim=1)).item()\n",
        "\n",
        "print(\"CNN Prediction:\", labels_map[pred])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GdbenDLkCwRf",
        "outputId": "d0a7dc5e-d9a1-4900-e484-6e40700269b0"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "===== MODEL COMPARISON =====\n",
            "\n",
            "Model  Accuracy  Precision  Recall  F1 Score\n",
            "  SVM    0.2122   0.190308  0.2122  0.146294\n",
            "  CNN    0.9166   0.916280  0.9166  0.916163\n"
          ]
        }
      ],
      "source": [
        "#comparison\n",
        "import torch\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "import pandas as pd\n",
        "\n",
        "# ======================================================\n",
        "# 1. Extract Encoder Features for Classical Model (SVM)\n",
        "# ======================================================\n",
        "encoder.eval()\n",
        "all_features = []\n",
        "all_labels = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for X, y in test_loader:\n",
        "        X = X.to(device)\n",
        "        feats = encoder(X).cpu().numpy()\n",
        "        all_features.extend(feats)\n",
        "        all_labels.extend(y.numpy())\n",
        "\n",
        "all_features = np.array(all_features)\n",
        "all_labels = np.array(all_labels)\n",
        "\n",
        "# ----- SVM Predictions -----\n",
        "svm_preds = svm.predict(all_features)\n",
        "\n",
        "svm_acc  = accuracy_score(all_labels, svm_preds)\n",
        "svm_prec = precision_score(all_labels, svm_preds, average='weighted')\n",
        "svm_rec  = recall_score(all_labels, svm_preds,   average='weighted')\n",
        "svm_f1   = f1_score(all_labels, svm_preds,       average='weighted')\n",
        "\n",
        "\n",
        "# ==========================================\n",
        "# 2. Evaluate CNN on Test Set\n",
        "# ==========================================\n",
        "model.eval()\n",
        "cnn_true = []\n",
        "cnn_pred = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for X, y in test_loader:\n",
        "        X = X.to(device)\n",
        "\n",
        "        outputs = model(X)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "\n",
        "        cnn_true.extend(y.numpy())\n",
        "        cnn_pred.extend(predicted.cpu().numpy())\n",
        "\n",
        "cnn_acc  = accuracy_score(cnn_true, cnn_pred)\n",
        "cnn_prec = precision_score(cnn_true, cnn_pred, average='weighted')\n",
        "cnn_rec  = recall_score(cnn_true, cnn_pred,   average='weighted')\n",
        "cnn_f1   = f1_score(cnn_true, cnn_pred,       average='weighted')\n",
        "\n",
        "\n",
        "# ==========================================\n",
        "# 3. Comparison Table\n",
        "# ==========================================\n",
        "results = pd.DataFrame({\n",
        "    \"Model\": [\"SVM\", \"CNN\"],\n",
        "    \"Accuracy\": [svm_acc, cnn_acc],\n",
        "    \"Precision\": [svm_prec, cnn_prec],\n",
        "    \"Recall\": [svm_rec, cnn_rec],\n",
        "    \"F1 Score\": [svm_f1, cnn_f1]\n",
        "})\n",
        "\n",
        "print(\"\\n===== MODEL COMPARISON =====\\n\")\n",
        "print(results.to_string(index=False))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Tk-Q6JfFNqB"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S6LeJCt4FPw4"
      },
      "outputs": [],
      "source": [
        "print(\"\"\"\n",
        "==================== FINAL CONCLUSION ====================\n",
        "\n",
        "ðŸ“Œ Model Performance Summary:\n",
        "- The CNN model achieved higher overall performance than SVM and Random Forest.\n",
        "- CNNs are better suited for image classification because they learn spatial\n",
        "  and hierarchical patterns directly from pixel data.\n",
        "\n",
        "âœ… Which model performed better?\n",
        "- The Convolutional Neural Network (CNN) performed better.\n",
        "\n",
        "âœ… Why did CNN perform better?\n",
        "- CNNs automatically extract strong visual features.\n",
        "- Classical ML models rely on the encoderâ€™s hand-crafted 32-dimensional features,\n",
        "  which are less expressive.\n",
        "\n",
        "âœ… Future Work:\n",
        "- Try deeper or pretrained CNN architectures (ResNet, VGG).\n",
        "- Apply data augmentation to improve generalization.\n",
        "- Tune hyperparameters of CNN and classical models.\n",
        "- Visualize learned features (PCA, t-SNE).\n",
        "- Test on more datasets or extend to color images.\n",
        "\n",
        "============================================================\n",
        "\"\"\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}